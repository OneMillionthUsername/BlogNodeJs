{
  "title": "Was sind Transformer?",
  "content": "<p>Ein <strong>Transformer</strong> ist ein spezieller <strong>neuronaler Netzwerk-Architekturtyp</strong>, der 2017 von Vaswani et al. im Paper &bdquo;Attention is All You Need&ldquo; eingef&uuml;hrt wurde. Er bildet die Grundlage f&uuml;r <strong>LLMs (Large Language Models)</strong> wie GPT, BERT, T5 usw.</p>\n<hr>\n<h3>1. <strong>Grundidee des Transformers</strong></h3>\n<p>Statt wie fr&uuml;here Modelle (RNNs, LSTMs) die W&ouml;rter einer Eingabesequenz nacheinander zu verarbeiten, verarbeitet der Transformer <strong>alle Tokens gleichzeitig</strong> (&bdquo;parallel&ldquo;). Er nutzt dabei ein zentrales Konzept namens:</p>\n<h3>2. <strong>Self-Attention (Selbstaufmerksamkeit)</strong></h3>\n<p>Hierbei schaut sich jedes Wort (bzw. Token) der Eingabe an, <strong>wie stark es mit anderen W&ouml;rtern zusammenh&auml;ngt</strong>. So kann das Modell z. B. bei einem Satz wie:</p>\n<blockquote>\n<p>&bdquo;Der Hund, der &uuml;ber die Wiese lief, bellte laut.&ldquo;</p>\n</blockquote>\n<p>verstehen, dass sich &bdquo;bellte&ldquo; auf &bdquo;Hund&ldquo; bezieht &ndash; auch wenn dazwischen viele W&ouml;rter stehen.</p>\n<hr>\n<h3>3. <strong>Bausteine des Transformers</strong></h3>\n<p>Ein Transformer besteht typischerweise aus mehreren <strong>Encoder-</strong> und/oder <strong>Decoder-Schichten</strong>:</p>\n<ul>\n<li>\n<p><strong>Encoder:</strong> Liest die Eingabesequenz und baut eine interne Repr&auml;sentation auf.</p>\n</li>\n<li>\n<p><strong>Decoder:</strong> Nimmt diese Repr&auml;sentation und erzeugt eine Ausgabesequenz (z. B. beim &Uuml;bersetzen).</p>\n</li>\n</ul>\n<p><strong>GPT</strong> verwendet nur Decoder-Schichten (autogenerativer Transformer),<br><strong>BERT</strong> verwendet nur Encoder-Schichten (bidirektionaler Kontext).</p>\n<hr>\n<h3>4. <strong>Komponenten im Detail</strong></h3>\n<ul>\n<li>\n<p><strong>Token-Embedding:</strong> Wandelt W&ouml;rter in Vektoren um.</p>\n</li>\n<li>\n<p><strong>Positionale Encodings:</strong> Da der Transformer keine Reihenfolge kennt, f&uuml;gt man Positionsinformationen hinzu.</p>\n</li>\n<li>\n<p><strong>Multi-Head Attention:</strong> F&uuml;hrt gleichzeitig mehrere Self-Attention-Berechnungen durch.</p>\n</li>\n<li>\n<p><strong>Feed-Forward-Netzwerk (MLP):</strong> Verarbeitet jedes Token unabh&auml;ngig weiter.</p>\n</li>\n<li>\n<p><strong>Layer Normalization und Residual-Verbindungen:</strong> Verbessern Stabilit&auml;t und Training.</p>\n</li>\n</ul>\n<hr>\n<h3>5. <strong>LLM = Transformer + viel Training</strong></h3>\n<p>Ein <strong>LLM (Large Language Model)</strong> ist ein Transformer-Modell mit:</p>\n<ul>\n<li>\n<p><strong>vielen Parametern</strong> (oft Milliarden)</p>\n</li>\n<li>\n<p><strong>Training auf riesigen Textmengen</strong></p>\n</li>\n<li>\n<p><strong>Feinabstimmung auf bestimmte Aufgaben</strong></p>\n</li>\n</ul>\n<p>Beispiele: <strong>GPT-4</strong>, <strong>Claude</strong>, <strong>Gemini</strong>, <strong>LLaMA</strong>, <strong>Mistral</strong>, <strong>DeepSeek</strong>, <strong>Command R</strong>, <strong>T5</strong>, usw.</p>\n<hr>\n<h3>6. <strong>Vorteile des Transformers</strong></h3>\n<ul>\n<li>\n<p>Parallelisierbar (schnelleres Training als RNNs)</p>\n</li>\n<li>\n<p>Lange Abh&auml;ngigkeiten erfassbar (Kontext &uuml;ber viele Tokens)</p>\n</li>\n<li>\n<p>Universell einsetzbar (Text, Bilder, Audio, Code)</p>\n</li>\n</ul>\n<hr>\n<p>&nbsp;</p>",
  "tags": [],
  "date": "2025-07-03T21:21:53.613Z"
}